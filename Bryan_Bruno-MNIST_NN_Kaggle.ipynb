{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Digit Recognizer Kaggle Competition Part 2</h1>\n",
    "<h2>Neural Nets with TensorFlow & Keras</h2>\n",
    "<h3>Bryan Bruno</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building Environment</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ft = train.iloc[:, 1:785]\n",
    "train_lbl = train.iloc[:, 0]\n",
    "X_test = test.iloc[:, 0:784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data for test and train\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_ft, train_lbl, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryan\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Bryan\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Bryan\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# continuing with the same split above while placing into matrix\n",
    "\n",
    "X_train = X_train.as_matrix().reshape(31500, 784) #.75\n",
    "X_val = X_val.as_matrix().reshape(10500, 784) #.25\n",
    "\n",
    "X_test = X_test.as_matrix().reshape(28000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing data\n",
    "\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_val = X_val.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting normalized data into categories for matrix allocation\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_val = keras.utils.to_categorical(y_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# printing values from index, will be either 0 or 1 to indicate the numeric value in matrix\n",
    "\n",
    "print(y_train[0], y_train[1], y_train[2])\n",
    "print(y_train[3], y_train[4], y_train[5])\n",
    "print(y_train[6], y_train[7], y_train[8])\n",
    "print(y_train[9], y_train[10], y_train[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural Network Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up standard params\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 400\n",
    "n_hidden2 = 300\n",
    "n_hidden3 = 200\n",
    "n_hidden4 = 100\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of computational time and the number of neural networks, only using a single epoch \n",
    "# this is for very basic benchmarking and will produce less accurate models than additional runs\n",
    "\n",
    "n_epochs = 1\n",
    "n_batch = 50\n",
    "sgd = optimizers.SGD(lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# four hidden layers using softmax\n",
    "\n",
    "Inp = Input(shape=(784,))\n",
    "l = Dense(n_hidden1, activation=\"relu\", name = \"hidden1\")(Inp)\n",
    "l = Dense(n_hidden2, activation=\"relu\", name = \"hidden2\")(l)\n",
    "l = Dense(n_hidden3, activation=\"relu\", name = \"hidden3\")(l)\n",
    "l = Dense(n_hidden4, activation=\"relu\", name = \"hidden4\")(l)\n",
    "output = Dense(n_outputs, activation = \"softmax\", name = \"outputs\")(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 400)               314000    \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 300)               120300    \n",
      "_________________________________________________________________\n",
      "hidden3 (Dense)              (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "hidden4 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "outputs (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 515,610\n",
      "Trainable params: 515,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# first neural network is built using stochastic gradient descent\n",
    "\n",
    "nn1 = Model(Inp, output)\n",
    "nn1.compile(loss = \"categorical_crossentropy\", optimizer = \"sgd\", metrics = [\"accuracy\"])\n",
    "nn1.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second neural network built with Adam\n",
    "\n",
    "l = Dense(n_hidden1, activation = \"relu\", name = \"hidden1\")(Inp)\n",
    "l = Dense(n_hidden2, activation = \"relu\", name = \"hidden2\")(l)\n",
    "l = Dense(n_hidden3, activation = \"relu\", name = \"hidden3\")(l)\n",
    "l = Dense(n_hidden4, activation = \"relu\", name = \"hidden4\")(l)\n",
    "output = Dense(n_outputs, activation = \"softmax\", name = \"outputs\")(l)\n",
    "\n",
    "adam = keras.optimizers.Adam(lr = 0.01)\n",
    "nn2 = Model(Inp, output)\n",
    "\n",
    "nn2.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third neural network built with Adam and two hidden layers\n",
    "\n",
    "l = Dense(n_hidden1, activation = \"relu\", name = \"hidden1\")(Inp)\n",
    "l = Dense(n_hidden4, activation = \"relu\", name = \"hidden4\")(l)\n",
    "output = Dense(n_outputs, activation = \"softmax\", name = \"outputs\")(l)\n",
    "\n",
    "adam = keras.optimizers.Adam(lr = 0.1)\n",
    "nn3 = Model(Inp, output)\n",
    "\n",
    "nn3.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth neural network built with Adam and five hidden layers\n",
    "\n",
    "l = Dense(n_hidden1, activation = \"relu\", name = \"hidden1\")(Inp)\n",
    "l = Dense(n_hidden2, activation = \"relu\", name = \"hidden2\")(l)\n",
    "l = Dense(n_hidden3, activation = \"relu\", name = \"hidden3\")(l)\n",
    "l = Dense(n_hidden4, activation = \"relu\", name = \"hidden4\")(l)\n",
    "l = Dense(n_hidden5, activation = \"relu\", name = \"hidden5\")(l)\n",
    "output = Dense(n_outputs, activation = \"softmax\", name = \"outputs\")(l)\n",
    "\n",
    "adam = keras.optimizers.Adam(lr = 0.01)\n",
    "nn4 = Model(Inp, output)\n",
    "\n",
    "nn4.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Nets Benchmark Experiment\n",
      "--------------------------------------------------------------------------\n",
      "Neural Net 1: Stochastic Gradient Descent\n",
      "0.1 Learing Rate | 4 Layers | Batches of 50\n",
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/1\n",
      " - 5s - loss: 1.0368 - acc: 0.7281 - val_loss: 0.4239 - val_acc: 0.8796\n",
      "\n",
      "Neural Net 2: Adam\n",
      "0.01 Learing Rate | 4 Layers | Batches of 50\n",
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.2755 - acc: 0.9172 - val_loss: 0.1508 - val_acc: 0.9534\n",
      "\n",
      "Neural Net 3: Adam\n",
      "0.1 Learing Rate | 2 Layers | Batches of 100\n",
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/1\n",
      " - 4s - loss: 0.3143 - acc: 0.9101 - val_loss: 0.1648 - val_acc: 0.9508\n",
      "\n",
      "Neural Net 4: Adam\n",
      "0.01 Learing Rate | 5 Layers | Batches of 50\n",
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/1\n",
      " - 8s - loss: 0.2917 - acc: 0.9123 - val_loss: 0.1527 - val_acc: 0.9544\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural Nets Benchmark Experiment\\n--------------------------------------------------------------------------\")\n",
    "print(\"Neural Net 1: Stochastic Gradient Descent\")\n",
    "print(\"0.1 Learing Rate | 4 Layers | Batches of 50\")\n",
    "nn1_fit = nn1.fit(X_train, y_train, batch_size = n_batch, verbose = 2,\n",
    "                   epochs = n_epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "print(\"\\nNeural Net 2: Adam\")\n",
    "print(\"0.01 Learing Rate | 4 Layers | Batches of 50\")\n",
    "nn2_fit = nn2.fit(X_train, y_train, batch_size = n_batch, verbose = 2,\n",
    "                   epochs = n_epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "print(\"\\nNeural Net 3: Adam\")\n",
    "print(\"0.1 Learing Rate | 2 Layers | Batches of 100\")\n",
    "nn3_fit = nn3.fit(X_train, y_train, batch_size = 100, verbose = 2,\n",
    "                   epochs = n_epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "print(\"\\nNeural Net 4: Adam\")\n",
    "print(\"0.01 Learing Rate | 5 Layers | Batches of 50\")\n",
    "nn4_fit = nn4.fit(X_train, y_train, batch_size = n_batch, verbose = 2,\n",
    "                   epochs = n_epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Benchmark Results Conclusion</h3>\n",
    "\n",
    "As we can see, there are some extremely disappointing results of all four neural networks. There are numerous items to discuss regarding these results. Neural networks are highly intricate in design, which allow for an immense amount of customization. This is often reflected through parameter tuning. \n",
    "\n",
    "The first item I’d like to bring up are the number of epochs allotted for testing purposes. A single epoch simple does not allow for enough validation and testing, resulting in highly underfit results. Conversely, increasing the number of epochs will eventually cause overfitting. The purpose is to train our NN, not for our model to memorize the data. As is, these models are grossly underfitting the data.\n",
    "\n",
    "The accuracies and error rates of each NN were very poor. The Stochastic Gradient Descent being far too abysmal to even consider tuning parameters for. While the Adam optimizer appeared to have more consistent results, the single epoch makes it very difficult to get an understanding of how well they would perform. \n",
    "\n",
    "There were simple adaptations to the three of these models. I found little significance to changing the learning rate from below the 0.1 value. The number of layers appeared to have some significance; however, more tests would need to be performed to verify this. From the very limited differences between these models, I found the most promise in the second NN iteration. The loss is relatively low while containing higher validation accuracy. This suggests that with more testing and especially the inclusion of additional epochs, it may perform significantly better.\n",
    "\n",
    "I’m intrigued enough to take the second NN and increase the epoch to five. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/5\n",
      "31500/31500 [==============================] - 9s 274us/step - loss: 0.2737 - acc: 0.9164 - val_loss: 0.1453 - val_acc: 0.9521\n",
      "Epoch 2/5\n",
      "31500/31500 [==============================] - 8s 256us/step - loss: 0.1102 - acc: 0.9654 - val_loss: 0.1662 - val_acc: 0.9514\n",
      "Epoch 3/5\n",
      "31500/31500 [==============================] - 8s 249us/step - loss: 0.0768 - acc: 0.9764 - val_loss: 0.1302 - val_acc: 0.9639\n",
      "Epoch 4/5\n",
      "31500/31500 [==============================] - 8s 247us/step - loss: 0.0611 - acc: 0.9803 - val_loss: 0.1153 - val_acc: 0.9682\n",
      "Epoch 5/5\n",
      "31500/31500 [==============================] - 8s 252us/step - loss: 0.0466 - acc: 0.9856 - val_loss: 0.1177 - val_acc: 0.9681\n"
     ]
    }
   ],
   "source": [
    "# demonstration of the second Adam model with five epochs\n",
    "\n",
    "l = Dense(n_hidden1, activation = \"relu\", name = \"hidden1\")(Inp)\n",
    "l = Dense(n_hidden2, activation = \"relu\", name = \"hidden2\")(l)\n",
    "l = Dense(n_hidden3, activation = \"relu\", name = \"hidden3\")(l)\n",
    "l = Dense(n_hidden4, activation = \"relu\", name = \"hidden4\")(l)\n",
    "output = Dense(n_outputs, activation = \"softmax\", name = \"outputs\")(l)\n",
    "\n",
    "adam = keras.optimizers.Adam(lr = 0.01)\n",
    "nn2 = Model(Inp, output)\n",
    "\n",
    "nn2.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "nn2_fit = nn2.fit(X_train, y_train, batch_size = n_batch, verbose = 1,\n",
    "                   epochs = 5, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, simply running five epochs significantly improved each performance category. It’s also worth noting the variance between this first iteration and the single iteration performed above. This is an incredibly important consideration to be aware of due to the validity of underfit models.\n",
    "\n",
    "While these results are much better and may be used in as a model, it’s important to remember that it was much simpler to achieve an accuracy score of around 0.96 using Random Forests. Not only was it simpler to implement and requires less computational time, but it performed better as well. Based on this, I cannot recommend any of these neural network models as replacement for Random Forests. However, I would choose the second NN optimized with Adam of these four.\n",
    "\n",
    "I’m not finished yet. Neural networks are immensely powerful, so I’m committed to build a new one from scratch and (hopefully) blow away Random Forests…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TensorFlow Neural Network</h3>\n",
    "<h4>Built for Competition!</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from scratch...\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 400\n",
    "n_hidden2 = 300\n",
    "n_hidden3 = 200\n",
    "n_hidden4 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = \"X\")\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name = \"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name = \"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "    hidden3 = neuron_layer(hidden2, n_hidden3, \"hidden3\", activation=\"relu\")\n",
    "    hidden4 = neuron_layer(hidden3, n_hidden4, \"hidden4\", activation=\"relu\")\n",
    "    logits = neuron_layer(hidden4, n_outputs, \"outputs\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate) \n",
    "    training_op = optimizer.minimize(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-058333efe22f>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Bryan\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Bryan\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\Bryan\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\Bryan\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Bryan\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 60 # some real epochs for learning!\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.98 Test accuracy: 0.9344\n",
      "1 Train accuracy: 0.96 Test accuracy: 0.9478\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.955\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9614\n",
      "4 Train accuracy: 0.98 Test accuracy: 0.9649\n",
      "5 Train accuracy: 0.96 Test accuracy: 0.9672\n",
      "6 Train accuracy: 0.98 Test accuracy: 0.9693\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9699\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9715\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.9735\n",
      "10 Train accuracy: 1.0 Test accuracy: 0.9728\n",
      "11 Train accuracy: 1.0 Test accuracy: 0.9737\n",
      "12 Train accuracy: 1.0 Test accuracy: 0.9744\n",
      "13 Train accuracy: 1.0 Test accuracy: 0.972\n",
      "14 Train accuracy: 1.0 Test accuracy: 0.9754\n",
      "15 Train accuracy: 1.0 Test accuracy: 0.9772\n",
      "16 Train accuracy: 1.0 Test accuracy: 0.9735\n",
      "17 Train accuracy: 1.0 Test accuracy: 0.9761\n",
      "18 Train accuracy: 1.0 Test accuracy: 0.9758\n",
      "19 Train accuracy: 1.0 Test accuracy: 0.9762\n",
      "20 Train accuracy: 1.0 Test accuracy: 0.9757\n",
      "21 Train accuracy: 1.0 Test accuracy: 0.9767\n",
      "22 Train accuracy: 1.0 Test accuracy: 0.9771\n",
      "23 Train accuracy: 1.0 Test accuracy: 0.977\n",
      "24 Train accuracy: 1.0 Test accuracy: 0.9771\n",
      "25 Train accuracy: 1.0 Test accuracy: 0.9771\n",
      "26 Train accuracy: 1.0 Test accuracy: 0.9773\n",
      "27 Train accuracy: 1.0 Test accuracy: 0.9766\n",
      "28 Train accuracy: 1.0 Test accuracy: 0.9773\n",
      "29 Train accuracy: 1.0 Test accuracy: 0.9764\n",
      "30 Train accuracy: 1.0 Test accuracy: 0.9773\n",
      "31 Train accuracy: 1.0 Test accuracy: 0.9774\n",
      "32 Train accuracy: 1.0 Test accuracy: 0.9775\n",
      "33 Train accuracy: 1.0 Test accuracy: 0.9775\n",
      "34 Train accuracy: 1.0 Test accuracy: 0.9771\n",
      "35 Train accuracy: 1.0 Test accuracy: 0.9772\n",
      "36 Train accuracy: 1.0 Test accuracy: 0.9767\n",
      "37 Train accuracy: 1.0 Test accuracy: 0.9775\n",
      "38 Train accuracy: 1.0 Test accuracy: 0.9776\n",
      "39 Train accuracy: 1.0 Test accuracy: 0.9776\n",
      "40 Train accuracy: 1.0 Test accuracy: 0.9774\n",
      "41 Train accuracy: 1.0 Test accuracy: 0.9776\n",
      "42 Train accuracy: 1.0 Test accuracy: 0.9771\n",
      "43 Train accuracy: 1.0 Test accuracy: 0.9775\n",
      "44 Train accuracy: 1.0 Test accuracy: 0.9777\n",
      "45 Train accuracy: 1.0 Test accuracy: 0.9778\n",
      "46 Train accuracy: 1.0 Test accuracy: 0.9774\n",
      "47 Train accuracy: 1.0 Test accuracy: 0.9772\n",
      "48 Train accuracy: 1.0 Test accuracy: 0.9769\n",
      "49 Train accuracy: 1.0 Test accuracy: 0.9777\n",
      "50 Train accuracy: 1.0 Test accuracy: 0.9776\n",
      "51 Train accuracy: 1.0 Test accuracy: 0.9778\n",
      "52 Train accuracy: 1.0 Test accuracy: 0.9774\n",
      "53 Train accuracy: 1.0 Test accuracy: 0.9769\n",
      "54 Train accuracy: 1.0 Test accuracy: 0.9779\n",
      "55 Train accuracy: 1.0 Test accuracy: 0.9772\n",
      "56 Train accuracy: 1.0 Test accuracy: 0.9777\n",
      "57 Train accuracy: 1.0 Test accuracy: 0.9775\n",
      "58 Train accuracy: 1.0 Test accuracy: 0.9774\n",
      "59 Train accuracy: 1.0 Test accuracy: 0.9772\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at thses scores! Much, much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    X_new_scaled = test[:]\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [2 0 9 ... 3 9 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted classes:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"testout.csv\", y_pred, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"ImageId\": list(range(1, len(y_pred) + 1)),\n",
    "              \"Label\": y_pred}).to_csv(\"testout.csv\", index = False, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update from original 1600 score w/ 60 epochs\n",
    "\n",
    "Submitted to Kaggle.com for a score of 0.99485.\n",
    "\n",
    "Rank: 525\n",
    "\n",
    "User ID: 2698396"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The \"Real\" Conclusion</h3>\n",
    "\n",
    "I think it goes without saying that I would highly recommend a Gradient Descent optimized Neural Network to identify hand written numbers. Gradient Descent is superior to Random Forests in the context of this data. The constant differentiable progression heavily minimizes loss. Additionally, there are other topics that need to be addressed. \n",
    "\n",
    "This recommendation stands when accuracy is the objective over computation time and resources. I can’t believe I forgot to time it in the code, but the 24 epochs took just under an hour to run. This is not a quick solution, but it is extremely accurate.\n",
    "\n",
    "On the same note, while the session was running, I was concerned for overfitting. At just about the halfway mark of the epochs, this NN hit a 1.0 accuracy score on the training data. I was very worried that this model had memorized the training data… However, I also noticed that the training scores started to slightly decrease while the test data continued to achieve higher accuracy scores. This allowed me to remain optimistic and run the model against the submission test data.\n",
    "\n",
    "I would love to run some more tests revolving around the number of epochs. The final results suggest that this model is not overfit and I would like to verify. As of now, I’m extremely satisfied with this score, but I may be coming back to this in the very near future!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
