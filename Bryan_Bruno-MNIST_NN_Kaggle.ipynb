{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "<h1>Digit Recognizer Kaggle Competition Part 2</h1>\n<h2>Neural Nets with TensorFlow & Keras</h2>\n<h3>Bryan Bruno</h3>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h3>Building Environment</h3>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf",
      "execution_count": 115,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split",
      "execution_count": 119,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import keras\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras import optimizers",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")",
      "execution_count": 125,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_ft = train.iloc[:, 1:785]\ntrain_lbl = train.iloc[:, 0]\nX_test = test.iloc[:, 0:784]",
      "execution_count": 126,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# splitting data for test and train\n\nX_train, X_val, y_train, y_val = train_test_split(train_ft, train_lbl, random_state = 12)",
      "execution_count": 130,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# continuing with the same split above while placing into matrix\n\nX_train = X_train.as_matrix().reshape(31500, 784) #.75\nX_val = X_val.as_matrix().reshape(10500, 784) #.25\n\nX_test = X_test.as_matrix().reshape(28000, 784)",
      "execution_count": 131,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# normalizing data\n\nX_train = X_train.astype(\"float32\")\nX_val = X_val.astype(\"float32\")\nX_test = X_test.astype(\"float32\")\n\nX_train /= 255\nX_val /= 255\nX_test /= 255",
      "execution_count": 137,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# converting normalized data into categories for matrix allocation\n\ny_train = keras.utils.to_categorical(y_train, 10)\ny_val = keras.utils.to_categorical(y_val, 10)",
      "execution_count": 139,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# printing values from index, will be either 0 or 1 to indicate the numeric value in matrix\n\nprint(y_train[0], y_train[1], y_train[2])\nprint(y_train[3], y_train[4], y_train[5])\nprint(y_train[6], y_train[7], y_train[8])\nprint(y_train[9], y_train[10], y_train[11])",
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h3>Neural Network Models</h3>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# setting up standard params\n\nn_inputs = 28*28\nn_hidden1 = 400\nn_hidden2 = 300\nn_hidden3 = 200\nn_hidden4 = 100\nn_hidden5 = 50\nn_outputs = 10",
      "execution_count": 224,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# because of computational time and the number of neural networks, only using a single epoch \n# this is for very basic benchmarking and will produce less accurate models than additional runs\n\nn_epochs = 1\nn_batch = 50\nsgd = optimizers.SGD(lr = 0.1)",
      "execution_count": 225,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# four hidden layers using softmax\n\nInp = Input(shape=(784,))\nl = Dense(n_hidden1, activation=\"relu\", name = \"hidden1\")(Inp)\nl = Dense(n_hidden2, activation=\"relu\", name = \"hidden2\")(l)\nl = Dense(n_hidden3, activation=\"relu\", name = \"hidden3\")(l)\nl = Dense(n_hidden4, activation=\"relu\", name = \"hidden4\")(l)\noutput = Dense(n_outputs, activation = \"softmax\", name = \"outputs\")(l)",
      "execution_count": 226,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# first neural network is built using stochastic gradient descent\n\nnn1 = Model(Inp, output)\nnn1.compile(loss = \"categorical_crossentropy\", optimizer = \"sgd\", metrics = [\"accuracy\"])\nnn1.summary() ",
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_11 (InputLayer)        (None, 784)               0         \n_________________________________________________________________\nhidden1 (Dense)              (None, 400)               314000    \n_________________________________________________________________\nhidden2 (Dense)              (None, 300)               120300    \n_________________________________________________________________\nhidden3 (Dense)              (None, 200)               60200     \n_________________________________________________________________\nhidden4 (Dense)              (None, 100)               20100     \n_________________________________________________________________\noutputs (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 515,610\nTrainable params: 515,610\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# second neural network built with Adam\n\nl = Dense(n_hidden1, activation = \"relu\", name = \"hidden1\")(Inp)\nl = Dense(n_hidden2, activation = \"relu\", name = \"hidden2\")(l)\nl = Dense(n_hidden3, activation = \"relu\", name = \"hidden3\")(l)\nl = Dense(n_hidden4, activation = \"relu\", name = \"hidden4\")(l)\noutput = Dense(n_outputs, activation = \"softmax\", name = \"outputs\")(l)\n\nadam = keras.optimizers.Adam(lr = 0.01)\nnn2 = Model(Inp, output)\n\nnn2.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])",
      "execution_count": 228,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# third neural network built with Adam and two hidden layers\n\nl = Dense(n_hidden1, activation = \"relu\", name = \"hidden1\")(Inp)\nl = Dense(n_hidden4, activation = \"relu\", name = \"hidden4\")(l)\noutput = Dense(n_outputs, activation = \"softmax\", name = \"outputs\")(l)\n\nadam = keras.optimizers.Adam(lr = 0.1)\nnn3 = Model(Inp, output)\n\nnn3.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])",
      "execution_count": 229,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# fourth neural network built with Adam and five hidden layers\n\nl = Dense(n_hidden1, activation = \"relu\", name = \"hidden1\")(Inp)\nl = Dense(n_hidden2, activation = \"relu\", name = \"hidden2\")(l)\nl = Dense(n_hidden3, activation = \"relu\", name = \"hidden3\")(l)\nl = Dense(n_hidden4, activation = \"relu\", name = \"hidden4\")(l)\nl = Dense(n_hidden5, activation = \"relu\", name = \"hidden5\")(l)\noutput = Dense(n_outputs, activation = \"softmax\", name = \"outputs\")(l)\n\nadam = keras.optimizers.Adam(lr = 0.01)\nnn4 = Model(Inp, output)\n\nnn4.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])",
      "execution_count": 230,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "print(\"Neural Nets Benchmark Experiment\\n--------------------------------------------------------------------------\")\nprint(\"Neural Net 1: Stochastic Gradient Descent\")\nprint(\"0.1 Learing Rate | 4 Layers | Batches of 50\")\nnn1_fit = nn1.fit(X_train, y_train, batch_size = n_batch, verbose = 2,\n                   epochs = n_epochs, validation_data=(X_val, y_val))\n\nprint(\"\\nNeural Net 2: Adam\")\nprint(\"0.01 Learing Rate | 4 Layers | Batches of 50\")\nnn2_fit = nn2.fit(X_train, y_train, batch_size = n_batch, verbose = 2,\n                   epochs = n_epochs, validation_data=(X_val, y_val))\n\nprint(\"\\nNeural Net 3: Adam\")\nprint(\"0.1 Learing Rate | 2 Layers | Batches of 100\")\nnn3_fit = nn3.fit(X_train, y_train, batch_size = 100, verbose = 2,\n                   epochs = n_epochs, validation_data=(X_val, y_val))\n\nprint(\"\\nNeural Net 4: Adam\")\nprint(\"0.01 Learing Rate | 5 Layers | Batches of 50\")\nnn4_fit = nn4.fit(X_train, y_train, batch_size = n_batch, verbose = 2,\n                   epochs = n_epochs, validation_data=(X_val, y_val))",
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Neural Nets Benchmark Experiement\n-------------------------------------------------------------------------\nNeural Net 1: Stochastic Gradient Descent\n0.1 Learing Rate | 4 Layers | Batches of 50\nTrain on 31500 samples, validate on 10500 samples\nEpoch 1/1\n - 199s - loss: 2.2998 - acc: 0.1257 - val_loss: 2.3174 - val_acc: 0.0898\n\nNeural Net 2: Adam\n0.01 Learing Rate | 4 Layers | Batches of 50\nTrain on 31500 samples, validate on 10500 samples\nEpoch 1/1\n - 209s - loss: 0.8782 - acc: 0.6960 - val_loss: 2.5715 - val_acc: 0.8370\n\nNeural Net 3: Adam\n0.1 Learing Rate | 2 Layers | Batches of 100\nTrain on 31500 samples, validate on 10500 samples\nEpoch 1/1\n - 125s - loss: 1.3001 - acc: 0.6221 - val_loss: 4.3250 - val_acc: 0.7179\n\nNeural Net 4: Adam\n0.01 Learing Rate | 5 Layers | Batches of 50\nTrain on 31500 samples, validate on 10500 samples\nEpoch 1/1\n - 225s - loss: 0.8578 - acc: 0.7176 - val_loss: 3.6992 - val_acc: 0.7657\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h3>Benchmark Results Conclusion</h3>\n\nAs we can see, there are some extremely disappointing results of all four neural networks. There are numerous items to discuss regarding these results. Neural networks are highly intricate in design, which allow for an immense amount of customization. This is often reflected through parameter tuning. \n\nThe first item I’d like to bring up are the number of epochs allotted for testing purposes. A single epoch simple does not allow for enough validation and testing, resulting in highly underfit results. Conversely, increasing the number of epochs will eventually cause overfitting. The purpose is to train our NN, not for our model to memorize the data. As is, these models are grossly underfitting the data.\n\nThe accuracies and error rates of each NN were very poor. The Stochastic Gradient Descent being far too abysmal to even consider tuning parameters for. While the Adam optimizer appeared to have more consistent results, the single epoch makes it very difficult to get an understanding of how well they would perform. \n\nThere were simple adaptations to the three of these models. I found little significance to changing the learning rate from below the 0.1 value. The number of layers appeared to have some significance; however, more tests would need to be performed to verify this. From the very limited differences between these models, I found the most promise in the second NN iteration. The loss is relatively low while containing higher validation accuracy. This suggests that with more testing and especially the inclusion of additional epochs, it may perform significantly better.\n\nI’m intrigued enough to take the second NN and increase the epoch to five. \n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# demonstration of the second Adam model with five epochs\n\nl = Dense(n_hidden1, activation = \"relu\", name = \"hidden1\")(Inp)\nl = Dense(n_hidden2, activation = \"relu\", name = \"hidden2\")(l)\nl = Dense(n_hidden3, activation = \"relu\", name = \"hidden3\")(l)\nl = Dense(n_hidden4, activation = \"relu\", name = \"hidden4\")(l)\noutput = Dense(n_outputs, activation = \"softmax\", name = \"outputs\")(l)\n\nadam = keras.optimizers.Adam(lr = 0.01)\nnn2 = Model(Inp, output)\n\nnn2.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n\nnn2_fit = nn2.fit(X_train, y_train, batch_size = n_batch, verbose = 1,\n                   epochs = 5, validation_data=(X_val, y_val))",
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 31500 samples, validate on 10500 samples\nEpoch 1/5\n31500/31500 [==============================] - 288s 9ms/step - loss: 0.8639 - acc: 0.7120 - val_loss: 3.5943 - val_acc: 0.7703\nEpoch 2/5\n31500/31500 [==============================] - 213s 7ms/step - loss: 0.3970 - acc: 0.8795 - val_loss: 2.9098 - val_acc: 0.8150\nEpoch 3/5\n31500/31500 [==============================] - 214s 7ms/step - loss: 0.2846 - acc: 0.9131 - val_loss: 1.7726 - val_acc: 0.8875\nEpoch 4/5\n31500/31500 [==============================] - 207s 7ms/step - loss: 0.2106 - acc: 0.9369 - val_loss: 1.3274 - val_acc: 0.9151\nEpoch 5/5\n31500/31500 [==============================] - 214s 7ms/step - loss: 0.1686 - acc: 0.9479 - val_loss: 1.0381 - val_acc: 0.9343\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As expected, simply running five epochs significantly improved each performance category. It’s also worth noting the variance between this first iteration and the single iteration performed above. This is an incredibly important consideration to be aware of due to the validity of underfit models.\n\nWhile these results are much better and may be used in as a model, it’s important to remember that it was much simpler to achieve an accuracy score of around 0.96 using Random Forests. Not only was it simpler to implement and requires less computational time, but it performed better as well. Based on this, I cannot recommend any of these neural network models as replacement for Random Forests. However, I would choose the second NN optimized with Adam of these four.\n\nI’m not finished yet. Neural networks are immensely powerful, so I’m committed to build a new one from scratch and (hopefully) blow away Random Forests…"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h3>TensorFlow Neural Network</h3>\n<h4>Built for Competition!</h4>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# starting from scratch...\n\nn_inputs = 28*28\nn_hidden1 = 400\nn_hidden2 = 300\nn_hidden3 = 200\nn_hidden4 = 100\nn_outputs = 10",
      "execution_count": 235,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = \"X\")\ny = tf.placeholder(tf.int64, shape = (None), name = \"y\")",
      "execution_count": 100,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def neuron_layer(X, n_neurons, name, activation=None):\n    with tf.name_scope(name):\n        n_inputs = int(X.get_shape()[1])\n        stddev = 2 / np.sqrt(n_inputs)\n        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n        W = tf.Variable(init, name = \"weights\")\n        b = tf.Variable(tf.zeros([n_neurons]), name = \"biases\")\n        z = tf.matmul(X, W) + b\n        if activation == \"relu\":\n            return tf.nn.relu(z)\n        else:\n            return z ",
      "execution_count": 106,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with tf.name_scope(\"dnn\"):\n    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n    hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n    hidden3 = neuron_layer(hidden2, n_hidden3, \"hidden3\", activation=\"relu\")\n    hidden4 = neuron_layer(hidden3, n_hidden4, \"hidden4\", activation=\"relu\")\n    logits = neuron_layer(hidden4, n_outputs, \"outputs\") ",
      "execution_count": 108,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with tf.name_scope(\"loss\"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n    loss = tf.reduce_mean(xentropy, name = \"loss\")",
      "execution_count": 109,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "learning_rate = 0.01\n\nwith tf.name_scope(\"train\"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate) \n    training_op = optimizer.minimize(loss) ",
      "execution_count": 110,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with tf.name_scope(\"eval\"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) ",
      "execution_count": 111,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "init = tf.global_variables_initializer()\nsaver = tf.train.Saver()",
      "execution_count": 112,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\") ",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Extracting /tmp/data/train-images-idx3-ubyte.gz\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "n_epochs = 24 # some real epochs for learning!\nbatch_size = 50",
      "execution_count": 114,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "with tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for iteration in range(mnist.train.num_examples // batch_size):\n            X_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n    save_path = saver.save(sess, \"./my_model_final.ckpt\")",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0 Train accuracy: 0.92 Test accuracy: 0.9002\n1 Train accuracy: 0.96 Test accuracy: 0.9192\n2 Train accuracy: 0.86 Test accuracy: 0.93\n3 Train accuracy: 0.96 Test accuracy: 0.9356\n4 Train accuracy: 0.94 Test accuracy: 0.9429\n5 Train accuracy: 0.94 Test accuracy: 0.9462\n6 Train accuracy: 0.96 Test accuracy: 0.9498\n7 Train accuracy: 0.98 Test accuracy: 0.9524\n8 Train accuracy: 0.94 Test accuracy: 0.9552\n9 Train accuracy: 0.98 Test accuracy: 0.9577\n10 Train accuracy: 0.98 Test accuracy: 0.9602\n11 Train accuracy: 1.0 Test accuracy: 0.961\n12 Train accuracy: 1.0 Test accuracy: 0.963\n13 Train accuracy: 1.0 Test accuracy: 0.9661\n14 Train accuracy: 0.98 Test accuracy: 0.9661\n15 Train accuracy: 1.0 Test accuracy: 0.9678\n16 Train accuracy: 0.96 Test accuracy: 0.9671\n17 Train accuracy: 0.98 Test accuracy: 0.9687\n18 Train accuracy: 0.98 Test accuracy: 0.9696\n19 Train accuracy: 0.98 Test accuracy: 0.9702\n20 Train accuracy: 1.0 Test accuracy: 0.9713\n21 Train accuracy: 0.98 Test accuracy: 0.9723\n22 Train accuracy: 0.98 Test accuracy: 0.9728\n23 Train accuracy: 1.0 Test accuracy: 0.973\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Look at thses scores! Much, much better!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test = pd.read_csv(\"test.csv\")",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "with tf.Session() as sess:\n    saver.restore(sess, \"./my_model_final.ckpt\")\n    X_new_scaled = test[:]\n    Z = logits.eval(feed_dict={X: X_new_scaled})\n    y_pred = np.argmax(Z, axis=1) ",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Predicted classes:\", y_pred)",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Predicted classes: [2 0 9 ... 3 9 2]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.savetxt(\"testout.csv\", y_pred, delimiter = \",\")",
      "execution_count": 81,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.DataFrame({\"ImageId\": list(range(1, len(y_pred) + 1)),\n              \"Label\": y_pred}).to_csv(\"testout.csv\", index = False, header = True)",
      "execution_count": 82,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Submitted to Kaggle.com for a score of 0.97814.\n\nRank: 1640\n\nUser ID: 2698396"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h3>The \"Real\" Conclusion</h3>\n\nI think it goes without saying that I would highly recommend a Gradient Descent optimized Neural Network to identify hand written numbers. Gradient Descent is superior to Random Forests in the context of this data. The constant differentiable progression heavily minimizes loss. Additionally, there are other topics that need to be addressed. \n\nThis recommendation stands when accuracy is the objective over computation time and resources. I can’t believe I forgot to time it in the code, but the 24 epochs took just under an hour to run. This is not a quick solution, but it is extremely accurate.\n\nOn the same note, while the session was running, I was concerned for overfitting. At just about the halfway mark of the epochs, this NN hit a 1.0 accuracy score on the training data. I was very worried that this model had memorized the training data… However, I also noticed that the training scores started to slightly decrease while the test data continued to achieve higher accuracy scores. This allowed me to remain optimistic and run the model against the submission test data.\n\nI would love to run some more tests revolving around the number of epochs. The final results suggest that this model is not overfit and I would like to verify. As of now, I’m extremely satisfied with this score, but I may be coming back to this in the very near future!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}